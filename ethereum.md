Multi-modal AI models, such as LLaVA and other vision-language architectures, have demonstrated impressive capabilities in tasks ranging from image captioning to medical diagnostics. However, these models often suffer from hallucination, where they generate incorrect or misleading information. In high-stakes applications like medical AI, hallucination can lead to misinterpretations of patient data, incorrect diagnoses, and a loss of trust in AI systems. My work focuses on developing efficient techniques to mitigate hallucination, particularly in medical contexts, by improving factual grounding, efficient adaptation, and uncertainty estimation.

Hallucination in multi-modal models occurs when they generate details not present in their input, often due to bias in training data, lack of grounding, or over-reliance on learned priors. Unlike text-only models, vision-language models can misinterpret images, produce inconsistent captions, or fail to align textual reasoning with visual input. This issue is particularly problematic in healthcare, where models trained on general datasets may introduce clinically inaccurate interpretations when applied to domain-specific electronic health record (EHR) data. Reducing hallucination requires stronger contextual grounding, parameter-efficient adaptation, and mechanisms for confidence-aware generation.

One effective technique for improving factual grounding is Retrieval-Augmented Generation (RAG), where the model retrieves relevant external documents before generating a response. This approach ensures that outputs are aligned with authoritative sources rather than overconfident hallucinations. In my research, I integrate retrieved EHR context into LLaVAâ€™s generation pipeline, conditioning the model on real patient data rather than pretraining biases. Another key approach is LoRA-based fine-tuning, which enables parameter-efficient domain adaptation by updating only small adapter layers instead of full model weights. This allows models to learn clinically relevant associations from datasets like MIMIC-IV-NOTE without excessive computational cost. I have applied Unsloth LoRA tuning to fine-tune multi-modal models on EHR text-image pairs, significantly improving the accuracy of AI-generated medical reports.

Beyond factual grounding and fine-tuning, another challenge is that AI models often do not recognize when they are uncertain. Many models generate high-confidence outputs even when incorrect, making them unreliable in critical applications. To address this, I am developing a token-based confidence scoring metric that assigns uncertainty values to generated outputs. By analyzing logit entropy and Bayesian uncertainty, the model can flag uncertain responses for human review, reducing the risks of incorrect AI-driven decisions in medical applications.

While these approaches improve reliability, hallucination remains a difficult challenge due to the lack of standardized evaluation metrics for multi-modal models. Unlike text-based hallucination, which can be measured through factual consistency scores, vision-language models require more complex benchmarks to assess misalignment between text and images. Future work should focus on better hallucination detection frameworks and long-context memory improvements, particularly in embodied AI and agent-based reasoning, where maintaining factual accuracy over multiple interactions is critical.

Reducing hallucination in multi-modal models is essential for ensuring the safe and effective deployment of AI in high-stakes environments. My work in retrieval grounding, efficient fine-tuning, and confidence-aware generation aims to improve factual reliability in medical AI and agent-based models. As AI continues to advance, solving hallucination will be a key step toward making AI more transparent, interpretable, and accessible to broader audiences, ensuring that trustworthy models can be widely adopted across research, healthcare, and beyond.